{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Training a chatbot.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nemoy2897/Chatbot286/blob/master/Training_a_chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "cptjvXImfMLs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import itertools\n",
        "\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tkB7yJtifiRa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "de8f2224-ea5a-4a0c-ba26-0a4cfbb72887"
      },
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34msample_data\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "GWYOpt6UqQ1S",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "outputId": "a1ef7f4d-61ab-43ff-b625-34e37098a427"
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-dd35b01f-1691-4dc1-8e54-aaaaa177e7fd\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-dd35b01f-1691-4dc1-8e54-aaaaa177e7fd\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving movie_conversations.txt to movie_conversations.txt\n",
            "Saving movie_lines.txt to movie_lines.txt\n",
            "User uploaded file \"movie_conversations.txt\" with length 6760930 bytes\n",
            "User uploaded file \"movie_lines.txt\" with length 34641919 bytes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Kk0aGIawrfpK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# !git clone https://github.com/suriyadeepan/datasets/blob/master/seq2seq/cornell_movie_corpus/scripts/prepare_data.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mOrBfPb6lQRb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "d5d78ca6-0ffb-44a1-baf4-7a2755b4fa7b"
      },
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from keras.layers import Input, Embedding, LSTM, TimeDistributed, Dense, Bidirectional\n",
        "from keras.models import Model, load_model\n",
        "\n",
        "INPUT_LENGTH = 20\n",
        "OUTPUT_LENGTH = 20"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "NpHKBpBJ7Yum",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#1.Opening Files with utf encoding "
      ]
    },
    {
      "metadata": {
        "id": "tM2aibAalTSk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "lines = open ('movie_lines.txt',encoding = 'utf-8', errors = 'ignore').read().split('\\n')\n",
        "conv_lines = open('movie_conversations.txt', encoding = 'utf-8',errors = 'ignore').read().split('\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NcxEHSM37j5r",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 2. Mapping Line id with dialogues"
      ]
    },
    {
      "metadata": {
        "id": "-PyzL1qandvj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "id2line = {}\n",
        "for line in lines:\n",
        "  _line = line.split(' +++$+++ ')\n",
        "  if len(_line) == 5:\n",
        "    id2line[_line[0]]=_line[4]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eBD9hYcMnwke",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "convs = []\n",
        "for line in conv_lines[:-1]:\n",
        "    _line = line.split(' +++$+++ ')[-1][1:-1].replace(\"'\",\"\").replace(\" \",\"\")\n",
        "    convs.append(_line.split(','))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "U5hBD8BJs3uA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "outputId": "a71b13a1-4f49-4f49-95d9-faa8630e9eb7"
      },
      "cell_type": "code",
      "source": [
        "for k in convs[298]:\n",
        "    print (k, id2line[k])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "L3464 ...delusions and paranoia.\n",
            "L3465 I was all of these.\n",
            "L3466 Well, you didn't appreciate the severity of it until recently.  No question about that.\n",
            "L3467 What about Oleg?\n",
            "L3468 Disappeared.  They're looking everywhere.  Maybe he went back to Czechoslovakia.\n",
            "L3469 No, he is here.  Shit...\n",
            "L3470 Don't worry about him.  Think about yourself.\n",
            "L3471 What about my movie rights?  Book rights?\n",
            "L3472 Look, I haven't really focused on that kind of thing.\n",
            "L3473 What's your cut?  How much?\n",
            "L3474 I would say...half.  Half is fair.\n",
            "L3475 No.  No way.\n",
            "L3476 But it's...\n",
            "L3477 Thirty-percent.  No more.  Or I call another lawyer.  This is the biggest case of your life.  Don't try to negotiate.  Thirty percent.  Say yes or no.\n",
            "L3478 This is not about money, Emil.  I need your trust in me.\n",
            "L3479 What else do you need?\n",
            "L3480 I need to know about your background.  I need to know about your upbringing.  Why you're here.\n",
            "L3481 Give me another one, please.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "whVS_n237vjJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 3. Splitting Dialogues to Q & A"
      ]
    },
    {
      "metadata": {
        "id": "xRustAZbtA5T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "1e6f4ada-be62-4924-b106-6b748d180445"
      },
      "cell_type": "code",
      "source": [
        "questions = []\n",
        "answers = []\n",
        "for conv in convs:\n",
        "  for i in range(len(conv)-1):\n",
        "    questions.append(id2line[conv[i]])\n",
        "    answers.append(id2line[conv[i+1]])\n",
        "    \n",
        "print(len(questions))\n",
        "print(len(answers))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "221616\n",
            "221616\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_QTtcPiJ73iK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 4. Clean the text and remove the shorthands and Symbols"
      ]
    },
    {
      "metadata": {
        "id": "Ey0X87BS4RUr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    '''Clean text by removing unnecessary characters and altering the format of words.'''\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"i'm\", \"i am\", text)\n",
        "    text = re.sub(r\"he's\", \"he is\", text)\n",
        "    text = re.sub(r\"she's\", \"she is\", text)\n",
        "    text = re.sub(r\"it's\", \"it is\", text)\n",
        "    text = re.sub(r\"that's\", \"that is\", text)\n",
        "    text = re.sub(r\"what's\", \"that is\", text)\n",
        "    text = re.sub(r\"where's\", \"where is\", text)\n",
        "    text = re.sub(r\"how's\", \"how is\", text)\n",
        "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
        "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
        "    text = re.sub(r\"\\'re\", \" are\", text)\n",
        "    text = re.sub(r\"\\'d\", \" would\", text)\n",
        "    text = re.sub(r\"\\'re\", \" are\", text)\n",
        "    text = re.sub(r\"won't\", \"will not\", text)\n",
        "    text = re.sub(r\"can't\", \"cannot\", text)\n",
        "    text = re.sub(r\"n't\", \" not\", text)\n",
        "    text = re.sub(r\"n'\", \"ng\", text)\n",
        "    text = re.sub(r\"'bout\", \"about\", text)\n",
        "    text = re.sub(r\"'til\", \"until\", text)\n",
        "    text = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|]\", \"\", text)\n",
        "    text = \" \".join(text.split())\n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qwu7kGfO47EO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Clean the data\n",
        "clean_questions = []\n",
        "for question in questions:\n",
        "    clean_questions.append(clean_text(question))\n",
        "clean_answers = []    \n",
        "for answer in answers:\n",
        "    clean_answers.append(clean_text(answer))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4Mn2iOmZ49Ow",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "0072affe-029f-4ce6-c9d6-24958c655fd7"
      },
      "cell_type": "code",
      "source": [
        "lengths = []\n",
        "\n",
        "for question in clean_questions:\n",
        "  lengths.append(len(question.split()))\n",
        "for answer in clean_answers:\n",
        "  lengths.append(len(answer.split()))\n",
        "  \n",
        "lengths= pd.DataFrame(lengths, columns = ['counts'])\n",
        "print(np.percentile(lengths,80))\n",
        "print(np.percentile(lengths,75))\n",
        "print(np.percentile(lengths,90))\n",
        "print(np.percentile(lengths,95))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "16.0\n",
            "14.0\n",
            "24.0\n",
            "32.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ADv9C_SjQt7R",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 5. Removing too long and too short Q & A"
      ]
    },
    {
      "metadata": {
        "id": "i9g1Cx7H63RB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "17c62865-bb04-4dcd-84d1-fb234331f560"
      },
      "cell_type": "code",
      "source": [
        "# Remove questions and answers that are shorter than 1 word and longer than 20 words.\n",
        "min_line_length = 2\n",
        "max_line_length = 20\n",
        "\n",
        "# Filter out the questions that are too short/long\n",
        "short_questions_temp = []\n",
        "short_answers_temp = []\n",
        "\n",
        "for i,question in enumerate(clean_questions):\n",
        "  if len(question.split()) >= min_line_length and len(question.split()) <= max_line_length:\n",
        "    short_questions_temp.append(question)\n",
        "    short_answers_temp.append(clean_answers[i])\n",
        "\n",
        "# Filter out the answers that are too short/long\n",
        "short_questions = []\n",
        "short_answers = []\n",
        "\n",
        "for i, answer in enumerate(short_answers_temp):\n",
        "    if len(answer.split()) >= min_line_length and len(answer.split()) <= max_line_length:\n",
        "        short_answers.append(answer)\n",
        "        short_questions.append(short_questions_temp[i])\n",
        "        \n",
        "print(len(short_questions))\n",
        "print(len(short_answers))\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "138528\n",
            "138528\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "GypjchJkQjpS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#6. Pre processing data for word model\n"
      ]
    },
    {
      "metadata": {
        "id": "rlPWgsmIQjl8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "TzsEWgZG_5lQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "bee711ff-f5ec-46e0-bd7e-a97e47031243"
      },
      "cell_type": "code",
      "source": [
        "num_samples  = 30000 #no. of samples to train on\n",
        "short_questions = short_questions[:num_samples]\n",
        "short_answers = short_answers[:num_samples]\n",
        "\n",
        "#tokenizing the ques and ans\n",
        "short_ques_tok = [nltk.word_tokenize(sent) for sent in short_questions] \n",
        "short_ans_tok = [nltk.word_tokenize(sent) for sent in short_answers]\n",
        "\n",
        "data_size = len(short_ques_tok)\n",
        "\n",
        "training_input = short_ques_tok[:round(data_size*(0.8))]\n",
        "training_input = [tr_input[::-1] for tr_input in training_input]\n",
        "\n",
        "training_output = short_ans_tok[:round(data_size*0.8)]\n",
        "\n",
        "validation_input = short_ques_tok[round(data_size*(80/100)):]\n",
        "validation_input  = [val_input[::-1] for val_input in validation_input] #reverseing input seq for better performance\n",
        "validation_output = short_ans_tok[round(data_size*(80/100)):]\n",
        "\n",
        "print('training size', len(training_input))\n",
        "print('validation size', len(validation_input))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training size 24000\n",
            "validation size 6000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZX2WM-hpScjq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from nltk.probability import FreqDist\n",
        "import itertools\n",
        "freqdist = FreqDist(itertools.chain(*(short_ques_tok+short_ans_tok)))\n",
        " \n",
        "\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ka_EgWm5U67h",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "vocab =dict( freqdist.most_common(1938))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mOh40RA-VDYU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "c3cb946e-f04e-4658-86aa-79c1deab2d9f"
      },
      "cell_type": "code",
      "source": [
        "print(vocab)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'.': 55097, 'you': 24017, ',': 22569, 'i': 21936, '?': 21324, 'the': 12485, 'is': 12190, 'not': 12160, 'to': 10352, 'it': 9977, 'a': 9334, 'do': 8046, 'that': 7769, '...': 6802, '!': 6299, 'are': 6282, 'what': 5770, 'me': 4984, 'have': 4899, 'of': 4662, 'and': 4271, 'in': 4263, 'am': 4187, 'we': 4170, 'he': 3781, 'will': 3451, 'know': 3336, 'this': 3335, 'for': 3295, 'your': 3211, 'no': 3137, 'my': 3125, 'was': 2917, \"'s\": 2879, 'can': 2792, 'be': 2706, 'on': 2587, 'did': 2520, 'just': 2474, 'would': 2281, 'like': 2202, 'with': 2192, 'but': 2120, 'here': 2076, 'about': 2068, 'they': 2038, 'how': 2028, 'get': 1970, 'there': 1937, 'all': 1924, 'want': 1854, 'so': 1848, 'got': 1782, 'think': 1751, 'if': 1670, 'out': 1668, 'well': 1657, 'go': 1621, 'she': 1620, 'oh': 1590, 'him': 1586, 'why': 1546, 'right': 1518, 'up': 1480, 'now': 1438, 'going': 1368, 'where': 1329, 'one': 1312, 'at': 1311, 'yes': 1245, 'who': 1221, 'see': 1209, 'yeah': 1203, 'good': 1113, 'her': 1111, 'come': 1018, 'say': 996, 'tell': 984, 'were': 976, 'let': 975, 'time': 956, 'been': 943, 'could': 911, 'as': 907, 'when': 888, 'then': 871, 'an': 869, 'some': 866, 'back': 843, 'look': 842, 'us': 823, 'mean': 818, 'from': 815, 'something': 813, 'never': 804, 'take': 801, 'his': 791, 'them': 771, 'really': 760, 'does': 760, 'too': 759, 'okay': 743, 'or': 738, 'man': 730, 'should': 687, 'na': 685, 'sure': 678, 'need': 667, 'way': 647, 'had': 647, 'any': 626, 'much': 619, 'very': 617, 'maybe': 608, 'said': 601, 'make': 586, 'little': 583, 'more': 581, 'over': 574, 'sorry': 555, 'doing': 553, 'down': 553, 'gon': 547, 'has': 534, 'nothing': 521, 'love': 519, 'only': 511, 'talk': 503, 'must': 497, 'anything': 496, 'please': 494, 'thought': 491, 'off': 486, 'by': 478, 'thing': 473, 'people': 470, 'our': 466, 'give': 466, 'two': 463, 'mr.': 461, 'thank': 457, 'better': 446, 'sir': 438, 'money': 437, 'work': 432, 'call': 432, 'god': 424, 'still': 422, 'long': 416, 'find': 413, 'told': 410, 'course': 403, 'help': 401, 'name': 400, 'believe': 396, 'because': 393, 'before': 390, 'hey': 389, 'guy': 387, 'home': 387, 'ever': 387, 'guess': 376, 'talking': 373, 'other': 372, 'first': 368, 'even': 363, 'life': 357, 'night': 355, 'last': 355, 'leave': 354, 'always': 353, 'hell': 353, 'shit': 351, 'wait': 345, 'these': 341, 'than': 339, 'keep': 338, 'again': 334, 'father': 331, 'remember': 328, 'wrong': 323, 'big': 322, 'everything': 322, 'into': 320, 'bad': 314, 'happened': 314, 'place': 314, 'those': 311, 'dead': 311, 'fucking': 310, 'ask': 309, 'feel': 309, 'kind': 304, 'around': 303, 'things': 302, 'fuck': 301, 'old': 300, 'else': 299, 'mind': 298, 'kill': 297, 'new': 296, 'girl': 293, 'fine': 293, 'away': 286, 'after': 286, 'great': 282, 'stay': 281, 'another': 280, 'ta': 280, 'put': 279, 'years': 278, 'might': 276, 'stop': 274, \"'\": 272, 'getting': 272, 'thanks': 271, 'day': 270, 'nice': 269, 'enough': 269, 'coming': 267, 'hear': 265, 'try': 264, 'understand': 251, 'three': 251, 'left': 250, 'lot': 249, 'done': 248, 'dad': 245, 'friend': 245, 'house': 245, 'seen': 241, 'uh': 240, 'real': 237, 'heard': 236, 'car': 235, 'looking': 235, 'wanted': 231, 'tonight': 229, 'saw': 228, 'came': 222, 'made': 221, 'room': 219, 'ai': 219, 'went': 218, 'huh': 218, 'boy': 216, 'yourself': 216, 'someone': 216, 'pretty': 215, 'many': 215, 'may': 214, 'care': 212, 'tomorrow': 211, 'job': 211, 'their': 210, 'yet': 209, 'jack': 209, 'morning': 207, 'knew': 205, 'mother': 205, 'mrs.': 203, 'same': 203, 'five': 202, 'already': 201, 'own': 201, 'through': 199, 'killed': 198, 'best': 198, 'idea': 195, 'says': 195, 'being': 194, 'stuff': 193, 'guys': 193, 'alone': 191, 'show': 191, 'minute': 189, 'listen': 188, 'hundred': 187, 'use': 187, 'called': 186, 'ya': 185, 'trying': 185, 'every': 182, 'live': 180, 'men': 180, 'four': 180, 'somebody': 180, 'business': 180, 'matter': 179, 'drink': 179, 'hello': 178, 'found': 177, 'next': 177, 'shut': 177, 'looks': 176, 'ten': 175, 'head': 172, 'woman': 172, 'mom': 172, 'thousand': 170, 'worry': 170, 'today': 169, 'world': 169, 'son': 168, 'without': 168, 'play': 168, 'anyway': 165, 'miss': 165, 'sleep': 162, 'saying': 162, 'start': 161, 'word': 161, 'actually': 160, 'since': 160, 'meet': 160, 'crazy': 160, 'afraid': 159, 'forget': 158, 'working': 158, 'knows': 157, 'probably': 157, 'doctor': 156, 'problem': 156, 'move': 156, 'baby': 156, 'once': 156, 'friends': 155, 'true': 154, \"'em\": 154, 'exactly': 153, 'while': 153, 'myself': 152, 'days': 152, 'used': 151, 'beautiful': 150, 'gone': 150, 'few': 150, 'point': 150, 'wife': 149, 'story': 148, 'wants': 148, 'late': 147, 'die': 146, 'anyone': 146, 'took': 146, 'together': 146, 'bring': 145, 'jesus': 145, 'married': 145, 'hi': 144, 'hope': 144, 'nobody': 142, 'most': 142, 'wish': 141, 'hold': 141, 'makes': 139, 'watch': 139, 'alright': 138, 'damn': 138, 'wan': 137, 'minutes': 137, 'school': 137, 'mine': 137, 'supposed': 136, 'such': 135, 'sit': 135, 'suppose': 135, 'whole': 134, 'thinking': 134, 'under': 133, 'dr.': 133, 'check': 133, 'hate': 132, 'eat': 132, 'deal': 132, 'run': 131, 'hurt': 131, 'ago': 130, 'open': 130, 'excuse': 128, 'kid': 128, 'dollars': 128, 'door': 128, 'game': 128, 'pay': 127, 'hard': 127, 'making': 126, 'lost': 126, 'shot': 126, 'ass': 124, 'half': 124, 'ready': 124, 'having': 123, 'which': 123, 'taking': 123, 'week': 123, 'happy': 123, 'both': 122, 'change': 122, 'trouble': 122, 'turn': 121, 'everybody': 121, 'trust': 120, 'read': 120, 'asked': 119, 'gun': 119, 'telling': 118, 'happen': 118, 'serious': 117, 'quite': 117, 'part': 116, 'stupid': 116, 'later': 116, 'anybody': 116, 'until': 115, 'soon': 115, 'whatever': 114, 'hand': 113, 'brother': 113, 'bob': 113, 'met': 112, 'waiting': 112, 'shall': 112, 'office': 111, 'ah': 110, 'dear': 110, 'bet': 110, 'either': 109, 'end': 108, 'year': 108, 'rest': 108, 'police': 108, 'almost': 108, 'least': 108, 'sick': 108, 'answer': 107, 'times': 107, 'far': 107, 'buy': 104, 'each': 104, 'party': 103, 'everyone': 103, 'death': 103, 'six': 103, 'young': 103, 'phone': 103, 'bed': 103, 'promise': 102, 'hours': 102, 'truth': 102, 'inside': 101, 'kids': 101, 'rick': 101, 'lady': 101, 'yours': 100, 'drive': 100, 'john': 100, 'scared': 100, 'easy': 99, 'frank': 99, 'christ': 99, 'speak': 98, 'harry': 97, \"c'mon\": 97, 'honey': 97, 'leaving': 97, 'second': 97, 'different': 96, 'case': 96, 'face': 96, 'anymore': 96, 'daddy': 95, 'war': 95, 'cut': 95, 'playing': 94, 'break': 94, 'fifty': 94, 'sound': 94, 'eyes': 94, 'family': 94, 'side': 94, 'shoot': 94, 'luck': 93, 'kidding': 92, 'bitch': 92, 'walk': 92, 'sounds': 92, 'asking': 91, 'set': 91, 'hot': 91, 'number': 91, 'funny': 91, 'heart': 91, 'question': 90, 'perhaps': 90, 'person': 89, 'black': 89, 'gave': 89, 'seems': 89, 'fire': 88, 'water': 88, 'movie': 88, 'follow': 88, 'couple': 87, 'women': 87, 'stand': 87, 'ship': 87, 'tried': 87, 'pick': 86, 'bit': 86, 'brought': 86, 'sometimes': 86, 'husband': 86, 'send': 85, 'hit': 85, 'important': 85, 'twenty': 84, 'news': 84, 'tired': 84, 'running': 84, 'though': 83, 'chance': 83, 'alive': 83, 'paul': 83, 'sex': 83, 'ray': 83, 'means': 82, 'comes': 82, 'goddamn': 82, 'town': 82, 'happens': 81, 'ahead': 81, 'gets': 80, 'york': 80, 'close': 80, 'hour': 80, 'lie': 79, 'eddie': 79, 'hotel': 79, 'daughter': 79, 'air': 79, 'body': 78, 'fun': 78, 'captain': 78, 'george': 78, 'hildy': 78, 'seem': 77, 'blood': 77, 'expect': 76, 'feeling': 76, 'somewhere': 76, 'goes': 75, 'along': 75, 'safe': 75, 'president': 75, 'against': 75, 'died': 75, 'anywhere': 75, 'food': 74, 'rather': 74, 'city': 74, 'dream': 74, 'date': 73, 'high': 73, 'early': 73, 'light': 73, 'outside': 73, 'moment': 73, 'david': 73, 'fair': 72, 'save': 72, 'living': 72, 'hands': 72, 'lucky': 72, 'finish': 72, 'also': 71, 'drop': 71, 'sort': 71, 'certainly': 71, 'fight': 71, 'full': 71, 'miles': 71, 'peter': 71, 'harold': 71, 'looked': 70, 'glad': 70, 'ride': 70, 'hurry': 70, 'nine': 70, 'boss': 70, 'choice': 69, 'words': 69, 'fast': 69, 'dinner': 69, 'eh': 69, 'poor': 69, 'wonderful': 69, 'plan': 68, 'interested': 68, 'dance': 68, 'started': 68, 'quit': 68, 'red': 68, 'till': 68, 'power': 68, 'happening': 68, 'reason': 67, 'lose': 67, 'bag': 67, 'king': 67, 'ben': 67, 'jake': 67, 'girls': 66, 'mad': 66, 'street': 66, 'country': 66, 'cold': 66, 'win': 65, 'parents': 65, 'sent': 65, 'dog': 65, 'elaine': 65, 'eight': 65, 'army': 65, 'dunno': 65, 'control': 64, 'front': 64, 'wonder': 64, 'free': 64, 'worth': 64, 'meeting': 64, 'human': 64, 'sweet': 63, 'busy': 63, 'west': 63, 'works': 63, 'behind': 63, 'hungry': 63, 'bullshit': 63, 'rob': 63, 'cool': 62, 'touch': 62, 'marry': 62, 'explain': 62, 'dignan': 62, 'swear': 61, 'book': 61, 'million': 61, 'possible': 61, 'months': 61, 'strange': 61, 'weeks': 60, 'worse': 60, 'darling': 60, 'takes': 60, 'straight': 60, 'thirty': 60, 'boys': 60, 'victor': 60, 'benjamin': 60, 'patrick': 59, 'bucks': 59, 'special': 59, 'questions': 59, 'report': 59, 'between': 59, 'baron': 59, 'blow': 58, 'needs': 58, 'seven': 58, 'taken': 58, 'coffee': 58, 'line': 58, 'sam': 58, 'felt': 57, 'except': 57, 'totally': 57, 'peel': 57, 'hair': 56, 'chief': 56, 'thinks': 56, 'mister': 56, 'yesterday': 56, 'goodbye': 56, 'worked': 56, 'lunch': 55, 'forgive': 55, 'fact': 55, 'calling': 55, 'evil': 55, 'terrible': 55, 'weird': 55, 'interesting': 54, 'besides': 54, 'beat': 54, 'picture': 54, 'known': 54, 'plane': 54, 'fifteen': 54, 'moving': 54, 'broke': 54, 'wake': 54, 'drunk': 54, 'company': 54, 'pleasure': 53, 'loved': 53, 'dark': 53, 'bruce': 53, 'himself': 52, 'fucked': 52, 'apartment': 52, 'ice': 52, 'class': 52, 'kiss': 52, 'general': 52, 'clothes': 52, 'careful': 52, 'officer': 52, 'dick': 52, 'upset': 51, 'small': 51, 'state': 51, 'shower': 51, 'killing': 51, 'bank': 51, 'smart': 51, 'perfect': 51, 'welcome': 51, 'difference': 51, 'fool': 51, 'quiet': 51, 'appreciate': 50, 'write': 50, 'pull': 50, 'lying': 50, 'owe': 50, 'evening': 50, 'feet': 50, 'asleep': 50, 'sense': 50, 'piece': 50, 'mozart': 50, 'order': 50, 'completely': 49, 'asshole': 49, 'law': 49, 'absolutely': 49, 'meant': 49, 'act': 49, 'earth': 49, 'mark': 49, 'sake': 49, \"ma'am\": 49, 'louis': 49, 'crap': 48, 'ring': 48, 'seeing': 48, 'fault': 48, 'cops': 48, 'its': 48, 'lord': 48, 'figure': 48, 'finally': 48, 'simple': 48, 'percent': 48, 'likes': 47, 'short': 47, 'slow': 47, 'spend': 47, 'hospital': 47, 'become': 47, 'learn': 47, 'ought': 47, 'less': 47, 'seconds': 47, 'sad': 47, 'clear': 47, 'children': 47, 'bill': 47, 'handle': 47, 'charlie': 47, 'deep': 47, 'unless': 46, 'imagine': 46, 'others': 46, 'holy': 46, 'murder': 46, \"o'clock\": 46, 'enjoy': 46, 'mary': 46, 'definitely': 46, 'none': 46, 'grand': 46, 'austin': 46, 'wear': 45, 'major': 45, 'tape': 45, 'lives': 45, 'voice': 45, 'herr': 45, 'mouth': 45, 'key': 45, 'catch': 45, 'kinda': 44, 'dress': 44, 'girlfriend': 44, 'outta': 44, 'grow': 44, 'wow': 44, 'future': 44, 'roy': 44, 'jeffrey': 44, 'rules': 43, 'movies': 43, 'rock': 43, 'service': 43, 'bathroom': 43, 'mommy': 43, 'private': 43, 'pardon': 43, 'debbie': 43, 'card': 43, 'near': 43, 'acting': 43, 'white': 43, 'liked': 43, 'sign': 43, 'robinson': 43, 'ma': 42, 'watching': 42, 'forgot': 42, 'tv': 42, 'join': 42, 'able': 42, 'bus': 42, 'finished': 42, 'walter': 42, 'jim': 42, 'weather': 42, 'music': 42, 'silly': 42, 'hang': 42, 'brian': 42, 'pete': 42, 'claude': 42, 'tough': 41, 'count': 41, 'situation': 41, 'detective': 41, 'information': 41, 'film': 41, 'age': 41, 'road': 41, 'involved': 41, 'beg': 41, 'majesty': 41, 'wearing': 41, 'mistake': 41, 'afternoon': 41, 'missed': 40, 'giving': 40, \"'cause\": 40, 'ha': 40, 'summer': 40, 'calm': 40, 'train': 40, 'bastard': 40, 'rich': 40, 'joke': 40, 'sister': 39, 'burn': 39, 'pain': 39, 'paid': 39, 'arrest': 39, 'english': 39, 'impossible': 39, 'cost': 39, 'fly': 39, 'lots': 39, 'bridge': 39, 'paper': 39, 'window': 39, 'alice': 39, 'accident': 39, 'church': 39, 'joe': 39, 'blue': 39, 'suit': 39, 'born': 39, 'ok': 39, 'pool': 39, 'homer': 39, 'josh': 39, 'ordell': 39, 'cute': 38, 'ones': 38, 'staying': 38, 'don': 38, 'certain': 38, 'stopped': 38, 'third': 38, 'birthday': 38, 'trip': 38, 'table': 38, 'um': 38, 'loves': 38, 'secret': 38, 'personal': 38, 'fall': 38, 'offer': 38, 'feels': 38, 'fear': 38, 'nervous': 38, 'forever': 37, 'neither': 37, 'child': 37, 'cause': 37, 'killer': 37, 'cop': 37, 'crew': 37, 'honest': 37, 'cole': 37, 'taste': 37, 'ted': 37, 'throw': 37, 'bother': 37, 'pass': 37, 'american': 37, 'standing': 37, 'smith': 37, 'gim': 37, 'reed': 37, 'kringelein': 37, 'marylin': 37, 'north': 36, 'french': 36, 'type': 36, 'strong': 36, 'longer': 36, 'insane': 36, 'lawyer': 36, 'america': 36, 'aw': 36, 'truck': 36, 'hardly': 36, 'fix': 36, 'eye': 36, 'mention': 36, 'wondering': 36, 'dangerous': 36, 'radio': 36, 'bomb': 36, 'named': 36, 'princess': 36, 'calls': 36, 'turned': 36, 'jail': 36, 'henry': 36, 'seemed': 35, 'missing': 35, 'de': 35, 'past': 35, 'holding': 35, 'middle': 35, 'sing': 35, 'dreams': 35, 'worried': 35, 'drinking': 35, 'song': 35, 'twice': 35, 'plenty': 35, 'marriage': 35, 'er': 35, 'doubt': 35, 'boat': 35, 'stephen': 35, 'arm': 35, 'store': 35, 'anthony': 35, 'jason': 35, 'favor': 34, 'devil': 34, 'visit': 34, 'island': 34, 'sun': 34, 'sell': 34, 'client': 34, 'floor': 34, 'gordon': 34, 'dying': 34, 'dude': 34, 'seat': 34, 'martin': 34, 'beaumont': 34, 'preysing': 34, 'lad': 34, 'figured': 33, 'discuss': 33, 'uncle': 33, 'bought': 33, 'cash': 33, 'land': 33, 'station': 33, 'clean': 33, 'machine': 33, 'single': 33, 'double': 33, 'talked': 33, 'commander': 33, 'computer': 33, 'protect': 33, 'fat': 33, 'matt': 33, 'al': 33, 'jesse': 33, \"y'know\": 33, 'neck': 33, 'picked': 33, 'marcus': 33, 'curious': 33, 'lived': 33, 'magic': 33, 'agree': 32, 'listening': 32, 'caught': 32, 'empty': 32, 'innocent': 32, 'quickly': 32, 'building': 32, 'dave': 32, 'saved': 32, 'quick': 32, 'awful': 32, 'surprised': 32, 'honor': 32, 'area': 32, 'field': 32, 'fish': 32, 'older': 32, 'wall': 32, 'vanessa': 32, 'eve': 32, 'williams': 32, 'goodnight': 32, 'relax': 32, 'lilly': 32, 'aye': 32, 'jones': 31, 'surely': 31, 'problems': 31, 'naw': 31, 'checked': 31, 'memory': 31, 'surprise': 31, 'mess': 31, 'won': 31, 'college': 31, 'tom': 31, 'obvious': 31, 'attack': 31, 'system': 31, 'jimmy': 31, 'airport': 31, 'fred': 31, 'beach': 31, 'price': 31, 'star': 31, 'scare': 31, 'madam': 31, 'green': 31, 'round': 31, 'blame': 31, 'crying': 31, 'frances': 31, 'dana': 31, 'normal': 30, 'fan': 30, 'attention': 30, 'band': 30, 'sarah': 30, 'whom': 30, 'crime': 30, 'guns': 30, 'tommy': 30, 'bar': 30, 'board': 30, 'keeping': 30, 'evan': 30, 'records': 30, 'dickson': 30, 'park': 30, 'reach': 30, 'south': 30, 'adam': 30, 'weir': 30, 'venkman': 30, 'reading': 29, 'helping': 29, 'respect': 29, 'cover': 29, 'driving': 29, 'usually': 29, 'partner': 29, 'seriously': 29, 'smoke': 29, 'brown': 29, 'moon': 29, 'stole': 29, 'interest': 29, 'art': 29, 'return': 29, 'message': 29, 'consider': 29, 'positive': 29, 'fell': 29, 'apart': 29, 'club': 29, 'television': 29, 'ways': 29, 'forty': 29, 'month': 29, 'ate': 29, 'cook': 29, 'soul': 29, 'stone': 29, 'upstairs': 29, 'record': 29, 'mantan': 29, 'rose': 29, 'mike': 29, 'howard': 29, 'ours': 29, 'seymour': 29, 'mollie': 29, 'teach': 28, 'extra': 28, 'eating': 28, 'realize': 28, 'mangs': 28, 'ocean': 28, 'putting': 28, 'broken': 28, 'books': 28, 'taught': 28, 'mission': 28, 'flight': 28, 'across': 28, 'brothers': 28, 'fit': 28, 'amazing': 28, 'test': 28, 'view': 28, 'charge': 28, 'cry': 28, 'emergency': 28, 'games': 28, 'gentlemen': 28, 'history': 28, 'twelve': 28, 'mostly': 28, 'powers': 28, 'stories': 28, 'moved': 28, 'keys': 28, 'data': 28, 'desk': 28, 'africa': 28, 'nathan': 28, 'favorite': 27, 'pig': 27, 'beer': 27, 'pictures': 27, 'position': 27, 'peace': 27, 'sweetheart': 27, 'evidence': 27, 'whose': 27, 'russian': 27, 'angry': 27, 'cheap': 27, 'space': 27, 'team': 27, 'locked': 27, 'immediately': 27, 'box': 27, 'recognize': 27, 'fellow': 27, 'tea': 27, 'idiot': 27, 'tight': 27, 'experience': 27, 'hanging': 27, 'orders': 27, 'proud': 27, 'court': 27, 'sergeant': 27, 'creature': 27, 'ran': 27, 'sixty': 27, 'grace': 27, 'animal': 27, 'changed': 27, 'stick': 27, 'breakfast': 27, 'smell': 27, 'monsieur': 27, 'steal': 27, 'gold': 27, 'casino': 27, 'hated': 26, 'places': 26, 'ridiculous': 26, 'command': 26, 'ms.': 26, 'disappeared': 26, 'department': 26, 'nah': 26, 'during': 26, 'awake': 26, 'buddy': 26, 'promised': 26, 'hide': 26, 'east': 26, 'relationship': 26, 'expected': 26, 'cross': 26, 'german': 26, 'lines': 26, 'rain': 26, 'yellow': 26, 'ok.': 26, 'donnie': 26, 'doolittle': 26, 'gods': 26, 'boyfriend': 25, 'queen': 25, 'insurance': 25, 'credit': 25, 'correct': 25, 'nay': 25, 'planning': 25, '$': 25, 'notice': 25, 'instead': 25, 'dumb': 25, 'expecting': 25, 'personally': 25, 'nature': 25, 'tear': 25, 'present': 25, 'nuts': 25, 'dirty': 25, 'lead': 25, 'eggs': 25, 'laugh': 25, 'push': 25, 'subject': 25, 'wrote': 25, 'london': 25, 'gee': 25, 'neighborhood': 25, 'beyond': 25, 'step': 25, 'ruth': 25, 'trap': 25, 'matters': 25, 'willie': 25, 'barry': 25, 'indy': 25, 'boring': 24, 'total': 24, 'starts': 24, 'group': 24, 'worst': 24, 'sleeping': 24, 'decided': 24, 'parts': 24, 'force': 24, 'forward': 24, 'training': 24, 'hiding': 24, 'drug': 24, 'stones': 24, 'gotten': 24, 'onto': 24, 'tells': 24, 'passed': 24, 'code': 24, 'paris': 24, 'regular': 24, 'fighting': 24, 'twombley': 24, 'agent': 24, 'lay': 24, 'interview': 24, 'walked': 24, 'lieutenant': 24, 'mama': 24, 'indeed': 24, 'papers': 24, 'uniform': 24, 'earlier': 24, 'lately': 24, 'trial': 24, 'travel': 24, 'kept': 24, 'gary': 24, 'dogs': 24, 'lab': 24, 'pants': 24, 'gift': 24, 'dorothy': 24, 'whoever': 24, 'pregnant': 24, 'closed': 24, 'obviously': 24, 'sitting': 24, 'spoke': 24, 'plays': 24, 'erik': 24, 'hill': 24, 'clay': 24, 'rex': 24, 'shark': 24, 'tis': 24, 'dry': 23, 'complete': 23, 'noticed': 23, 'dollar': 23, 'prefer': 23, 'chicken': 23, 'believed': 23, 'raise': 23, 'famous': 23, 'scene': 23, 'madison': 23, 'opinion': 23, 'color': 23, 'advice': 23, 'hal': 23, 'sooner': 23, 'dancing': 23, 'cat': 23, 'license': 23, 'knowing': 23, 'language': 23, 'accept': 23, 'using': 23, 'share': 23, 'teeth': 23, 'apologize': 23, 'entire': 23, 'otherwise': 23, 'easier': 23, 'admit': 23, 'screw': 23, 'biggest': 23, 'decision': 23, 'herself': 23, 'pity': 23, 'roger': 23, 'concerned': 23, 'plans': 23, 'risk': 23, 'escape': 23, 'dump': 23, 'christmas': 23, 'sold': 23, 'rough': 23, 'slept': 23, 'earl': 23, 'nick': 23, 'rule': 23, 'needed': 23, 'wally': 23, 'note': 23, 'build': 23, 'built': 23, 'fats': 23, 'telly': 23, 'study': 22, 'direct': 22, 'familiar': 22, 'given': 22, 'letters': 22, 'waited': 22, 'attorney': 22, 'everywhere': 22, 'suspect': 22, 'carrying': 22, 'carry': 22, 'max': 22, 'low': 22, 'size': 22, 'walking': 22, 'danger': 22, 'knock': 22, 'lock': 22, 'skipper': 22, 'carl': 22, 'shooting': 22, 'ground': 22, 'pop': 22, 'colonel': 22, 'excited': 22, 'dressed': 22, 'glass': 22, 'bachelor': 22, 'conversation': 22, 'larry': 22, 'heavy': 22, 'sea': 22, 'match': 22, 'ladies': 22, 'dunbar': 22, 'michael': 22, 'damned': 22, 'hole': 22, 'belongs': 22, 'drove': 22, 'berlin': 22, 'laszlo': 22, 'merrick': 22, 'map': 22, \"o'neil\": 22, 'starting': 21, 'william': 21, 'soft': 21, 'heat': 21, 'duty': 21, 'bodies': 21, 'charming': 21, 'buried': 21, 'doors': 21, 'san': 21, 'bail': 21, 'losing': 21, 'vacation': 21, 'murdered': 21, 'offense': 21, 'divorced': 21, 'tickets': 21, 'played': 21, 'sonofabitch': 21, 'hunting': 21, 'freezing': 21, 'sensitive': 21, 'proper': 21, 'heading': 21, 'toilet': 21, 'blew': 21, 'cleaner': 21, 'cup': 21, 'keeps': 21, 'feed': 21, 'warm': 21, 'letter': 21, 'farm': 21, 'dozen': 21, 'spot': 21, 'van': 21, 'list': 21, 'harvard': 21, 'often': 21, 'main': 21, 'natural': 21, 'laura': 21, 'support': 21, 'picking': 21, 'ticket': 21, 'pray': 21, 'grab': 21, 'sky': 21, 'pack': 21, 'horses': 21, 'treves': 21, 'shaw': 21, 'dating': 20, 'falls': 20, 'hates': 20, 'friday': 20, 'copy': 20, 'effect': 20, 'truly': 20, 'address': 20, 'lied': 20, 'due': 20, 'nearly': 20, 'gay': 20, 'cousin': 20, 'bones': 20, 'lovely': 20, 'base': 20, 'fill': 20, 'wet': 20, 'weapon': 20, 'smile': 20, 'prove': 20, 'decide': 20, 'storm': 20, 'fired': 20, 'wade': 20, 'guard': 20, 'lights': 20, 'sixteen': 20, 'begin': 20, 'trunk': 20, 'vienna': 20, 'opera': 20, 'post': 20, 'sire': 20, 'hearing': 20, 'congratulations': 20, 'wolf': 20, 'england': 20, 'winston': 20, 'kick': 20, 'liar': 20, 'horse': 20, 'jam': 20, 'sheriff': 20, 'shame': 20, 'james': 20, 'andy': 20, 'hmmm': 20, 'hoping': 20, 'further': 20, 'mayor': 20, 'battle': 20, 'sword': 20, 'cake': 20, 'cards': 20, 'disease': 20, 'faith': 20, 'alarm': 20, 'bird': 20, 'richard': 20, 'expert': 20, 'casablanca': 20, 'pounds': 20, 'moves': 20, 'ryan': 20, 'juno': 20, 'damage': 19, 'track': 19, 'following': 19, 'awhile': 19, 'blind': 19, 'simply': 19, 'beginning': 19, 'contact': 19, 'judge': 19, 'medicine': 19, 'drag': 19, 'tree': 19, 'condition': 19, 'split': 19, 'likely': 19, 'within': 19, 'confidence': 19, 'throat': 19, 'deliver': 19, 'file': 19, 'necessary': 19, 'hero': 19, 'pulled': 19, 'waste': 19, 'tricks': 19, 'wind': 19, 'angel': 19, 'aboard': 19, 'weekend': 19, 'california': 19, 'sheila': 19, 'lonely': 19, 'stuck': 19, 'pilot': 19, 'defense': 19, 'large': 19, 'paint': 19, 'exciting': 19, 'especially': 19, 'forced': 19, 'slaughtered': 19, 'grade': 19, 'pike': 19, 'perfectly': 19, 'attractive': 19, 'lee': 19, 'river': 19, 'owen': 19, 'depends': 19, 'reasonable': 19, 'milo': 19, 'glasses': 19, 'supplies': 19, 'event': 19, 'cares': 19, 'castle': 19, 'mueller': 19, 'buying': 19, 'science': 19, 'scale': 19, 'tests': 19, 'collection': 19, 'twentyfive': 19, 'storage': 19, 'joel': 19, 'congress': 19, 'ii': 19, 'bert': 19, 'jackie': 19, 'gump': 19, 'joey': 18, 'repeat': 18, 'nope': 18, 'balls': 18, 'kitchen': 18, 'press': 18, 'burned': 18, 'suspicious': 18, 'ganz': 18, 'details': 18, 'player': 18, 'bear': 18, 'bunch': 18, 'wallet': 18, 'destroy': 18, 'warrior': 18, 'learned': 18, 'burns': 18, 'cell': 18, 'freak': 18, 'solid': 18, 'runs': 18, 'smoking': 18, 'harder': 18, 'flying': 18, 'trade': 18, 'ugly': 18, 'files': 18, 'policeman': 18, 'assume': 18, 'bedroom': 18, 'particularly': 18, 'top': 18, 'warn': 18, 'exist': 18, 'terrific': 18, 'turning': 18, 'mentioned': 18, 'writing': 18, 'alex': 18, 'thief': 18, 'eleven': 18, 'st.': 18, 'difficult': 18, 'ruin': 18, 'degrees': 18, 'frozen': 18, 'possibly': 18, 'throwing': 18, 'manager': 18, 'ball': 18, 'bringing': 18, 'public': 18, 'yo': 18, 'shoes': 18, 'chair': 18, 'pair': 18, 'midnight': 18, 'jeff': 18, 'brilliant': 18, 'loose': 18, 'worm': 18, 'willing': 18, 'lem': 18, 'enid': 18, 'justin': 18, 'meal': 18, 'prepared': 18, 'penitent': 18, 'massey': 18, 'eddy': 18, 'bianca': 17, 'babe': 17, 'decent': 17, 'signed': 17, 'legs': 17, 'wild': 17, 'british': 17, 'considered': 17, 'heaven': 17, 'i.': 17, 'camera': 17, 'shows': 17, 'prison': 17, 'gang': 17, 'permission': 17, 'basic': 17, 'finger': 17, 'cab': 17, 'nightmare': 17, 'desert': 17, 'saturday': 17, 'treat': 17, 'zero': 17, 'watched': 17, 'plus': 17, 'master': 17, 'dan': 17, 'deserves': 17, 'systems': 17, 'ourselves': 17, 'fresh': 17, 'ballet': 17, 'sweetie': 17, 'justice': 17, 'suicide': 17, 'speech': 17, 'bright': 17, 'sexual': 17, 'patient': 17, 'assistant': 17, 'warren': 17, 'somehow': 17, 'backwards': 17, 'common': 17, 'emperor': 17, 'choose': 17, 'per': 17, 'excellency': 17, 'wedding': 17, 'zoo': 17, 'cluett': 17, 'invite': 17, 'railroad': 17, 'hall': 17, 'loud': 17, 'bateman': 17, 'mood': 17, 'tie': 17, 'barely': 17, 'jump': 17, 'penis': 17, 'research': 17, 'seventyfive': 17, 'tits': 17, 'sloan': 17, 'nose': 17, 'fortune': 17, 'search': 17, 'incredible': 17, 'nowhere': 17, 'tuesday': 17, 'bye': 17, 'touched': 17, 'episode': 17, 'garage': 17, 'beloved': 17, 'breaking': 17, 'mexico': 17, 'brad': 17, 'hamilton': 17, 'arctic': 17, 'animals': 17, 'bobo': 17, 'tod': 17, 'prenup': 17, 'adele': 17, 'lyssa': 17, 'prom': 16, 'piss': 16, 'unfortunately': 16, 'remind': 16, 'drugs': 16, 'jealous': 16, 'whether': 16, 'security': 16, 'stomach': 16, 'style': 16, 'mr': 16, 'level': 16, 'oughta': 16, 'paranoid': 16, 'reasons': 16, 'followed': 16, 'mixed': 16, 'became': 16, 'pie': 16, 'offered': 16, 'cheese': 16, '&': 16, 'served': 16, 'rooms': 16, 'ear': 16, 'downtown': 16, 'health': 16, 'roll': 16, 'energy': 16, 'signal': 16, 'newt': 16, 'paying': 16, 'rome': 16, 'continue': 16, 'clark': 16, 'planned': 16, 'joint': 16, 'sisters': 16, 'unusual': 16, 'drinks': 16, 'loser': 16, 'tip': 16, 'guest': 16, 'rate': 16, 'freedom': 16, 'protection': 16, 'brains': 16, 'chip': 16, 'showed': 16, 'convention': 16, 'roof': 16, 'virgin': 16, 'laid': 16, 'fucker': 16, 'visa': 16, 'psychic': 16, 'survive': 16, 'sometime': 16, 'hmm': 16, 'manners': 16, 'vessel': 16, 'fourteen': 16, 'caesar': 16, 'suzette': 16, 'shuffleboard': 16, 'conor': 16, 'felson': 16, 'lowell': 16, 'oswald': 16, 'kay': 16, 'gentleman': 15, 'becoming': 15, 'starving': 15, 'above': 15, 'foot': 15, 'beast': 15, 'rid': 15, 'appointment': 15, 'talent': 15, 'rights': 15, 'da': 15, 'process': 15, 'thirsty': 15, 'gives': 15, 'bags': 15, 'spent': 15, 'crowd': 15, 'strength': 15, 'unit': 15, 'section': 15, 'candy': 15, 'action': 15, 'pieces': 15, 'bell': 15, 'explanation': 15, 'commission': 15, 'selling': 15, 'stiff': 15, 'dig': 15, 'admire': 15, 'divorce': 15, 'kristen': 15, 'stayed': 15, 'responsibility': 15, 'grave': 15, 'weapons': 15, 'form': 15, 'chest': 15, 'mac': 15, 'eighteen': 15, 'responsible': 15, 'afford': 15, 'shuttle': 15, 'opened': 15, 'vacuum': 15, 'avenue': 15, 'arrived': 15, 'impressed': 15, 'babies': 15, 'director': 15, 'marvelous': 15, 'papa': 15, 'woke': 15, 'pal': 15, 'alibi': 15, 'miller': 15, 'deserve': 15, 'downstairs': 15, 'naked': 15, 'glory': 15, 'showing': 15, 'drank': 15, 'duke': 15, 'baseball': 15, 'names': 15, 'fallen': 15, 'breath': 15, 'conklin': 15, 'coat': 15, 'locker': 15, 'answers': 15, 'lucy': 15, 'sandy': 15, 'rape': 15, 'threaten': 15, 'bored': 15, 'sale': 15, 'cigarette': 15, 'talby': 15, 'marks': 15, 'horrible': 15, 'farmer': 15, 'horn': 15, 'metal': 15, 'pressure': 15, 'knife': 15, 'probation': 15, 'cadillac': 15, 'karen': 15, 'scream': 15, 'wire': 15, 'demons': 15, 'gruner': 15, 'wigand': 15, 'rexroth': 15, 'bertrand': 15, 'windham': 15}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "I6uXlHXRWJ7T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cfcfe093-1fe3-4e94-a347-83e391a2376e"
      },
      "cell_type": "code",
      "source": [
        "#we will create dictionaries to provide a unique integer for each word.\n",
        "WORD_CODE_START = 1\n",
        "WORD_CODE_PADDING = 0\n",
        "\n",
        "word_num =2\n",
        "encoding  = {}\n",
        "decoding = {1:'START'}\n",
        "threshold =15\n",
        "for word, count in vocab.items():\n",
        "  if count >= threshold:\n",
        "    encoding[word] = word_num\n",
        "    decoding[word_num] =word\n",
        "    word_num+=1\n",
        "\n",
        "print(\"No. of vocab used:\", word_num)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No. of vocab used: 1940\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ocqj2uULYAc5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "decoding[len(encoding)+2]='<UNK>'\n",
        "encoding['<UNK>'] = len(encoding)+2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GQV-gEa1YqI6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dict_size = word_num +1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZISieJW3wAge",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 7. vectoring dataset"
      ]
    },
    {
      "metadata": {
        "id": "Kx3VGah5v5Ce",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def transform(encoding, data, vector_size =20):\n",
        "  transformed_data = np.zeros(shape=(len(data), vector_size))\n",
        "  for i in range(len(data)):\n",
        "    for j in range(min(len(data[i]), vector_size)):\n",
        "      try:\n",
        "        transformed_data[i][j] = encoding[data[i][j]]\n",
        "      except:\n",
        "        transformed_data[i][j] = encoding['<UNK>']\n",
        "  return transformed_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WDLjSeFcxrkD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "9174ebc2-1ab2-4397-a712-15019e3c9dfc"
      },
      "cell_type": "code",
      "source": [
        "enc_train_input = transform(encoding, training_input,vector_size= INPUT_LENGTH)\n",
        "\n",
        "enc_train_output = transform(encoding, training_output,vector_size=OUTPUT_LENGTH)\n",
        "\n",
        "print('encoded training input', enc_train_input.shape)\n",
        "print('encoded training output', enc_train_output.shape)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "encoded training input (24000, 20)\n",
            "encoded training output (24000, 20)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0Ue3NAh3_h62",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "0317ca22-5353-48a6-cb49-b4c137d4c85b"
      },
      "cell_type": "code",
      "source": [
        "enc_valid_input =  transform(encoding, validation_input, vector_size=INPUT_LENGTH)\n",
        "\n",
        "enc_valid_output = transform(encoding,validation_output,vector_size = OUTPUT_LENGTH)\n",
        "\n",
        "print('encoded validation input', enc_valid_input.shape)\n",
        "print('encoded validation output', enc_valid_output.shape)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "encoded validation input (6000, 20)\n",
            "encoded validation output (6000, 20)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0Krp-FKHyzRK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Model Building\n",
        "##Sequence to Sequence Models"
      ]
    },
    {
      "metadata": {
        "id": "kWkxHUEm0lBN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tf.keras.backend.clear_session()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2IA2TK4xyduF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "INPUT_LENGTH =20\n",
        "OUTPUT_LENGTH =20\n",
        "\n",
        "enc_input = Input(shape=(INPUT_LENGTH,))\n",
        "dec_input = Input(shape=(OUTPUT_LENGTH,))\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VxZXk0j40yhA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "338d9463-1b46-46b2-a87e-715f64e22f08"
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import SimpleRNN\n",
        "\n",
        "encoder = Embedding(dict_size, 128, input_length=INPUT_LENGTH, mask_zero=True)(enc_input)\n",
        "\n",
        "encoder = LSTM(512, return_sequences = True, unroll= True)(encoder)\n",
        "\n",
        "encoder_last = encoder[:,-1,:]\n",
        "\n",
        "print('encoder', encoder)\n",
        "print('encoder_last', encoder_last)\n",
        "\n",
        "decoder = Embedding(dict_size, 128, input_length=OUTPUT_LENGTH, mask_zero=True)(dec_input)\n",
        "\n",
        "decoder = LSTM(512, return_sequences = True, unroll=True)(decoder, initial_state=[encoder_last, encoder_last])\n",
        "\n",
        "print('decoder', decoder)\n",
        "\n",
        "# For the plain Sequence-to-Sequence, we produced the output from directly from decoder\n",
        "# output = TimeDistributed(Dense(output_dict_size, activation=\"softmax\"))(decoder)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "encoder Tensor(\"lstm_1/transpose_2:0\", shape=(?, 20, 512), dtype=float32)\n",
            "encoder_last Tensor(\"strided_slice:0\", shape=(?, 512), dtype=float32)\n",
            "decoder Tensor(\"lstm_2/transpose_2:0\", shape=(?, 20, 512), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uy_TMou_2LvF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Attention Mechanism"
      ]
    },
    {
      "metadata": {
        "id": "_rq3D_x815qT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "5bd8f8c8-137c-47fb-9574-691575240d0c"
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import Activation, dot, concatenate\n",
        "\n",
        "attention = dot([decoder,encoder], axes = [2,2])\n",
        "attention = Activation('softmax', name='attention')(attention)\n",
        "print('attention', attention)\n",
        "\n",
        "context = dot([attention, encoder], axes =[2,1])\n",
        "print('context', context)\n",
        "\n",
        "decoder_combined_context= concatenate([context,decoder])\n",
        "print('decoder_combined_context', decoder_combined_context)\n",
        "\n",
        "output = TimeDistributed(Dense(512, activation='tanh'))(decoder_combined_context)\n",
        "output = TimeDistributed(Dense(dict_size, activation='softmax'))(output)\n",
        "print('output', output)\n"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "attention Tensor(\"attention/truediv:0\", shape=(?, 20, 20), dtype=float32)\n",
            "context Tensor(\"dot_2/MatMul:0\", shape=(?, 20, 512), dtype=float32)\n",
            "decoder_combined_context Tensor(\"concatenate_1/concat:0\", shape=(?, 20, 1024), dtype=float32)\n",
            "output Tensor(\"time_distributed_2/Reshape_1:0\", shape=(?, 20, 1941), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "fAzzqfKF3hJY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        },
        "outputId": "1614adf4-dbae-4097-e900-c4dad28222a2"
      },
      "cell_type": "code",
      "source": [
        "model= Model(inputs=[enc_input, dec_input], outputs=[output])\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "model.summary()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            (None, 20)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_1 (InputLayer)            (None, 20)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_2 (Embedding)         (None, 20, 128)      248448      input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, 20, 128)      248448      input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm_2 (LSTM)                   (None, 20, 512)      1312768     embedding_2[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 20, 512)      1312768     embedding_1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dot_1 (Dot)                     (None, 20, 20)       0           lstm_2[0][0]                     \n",
            "                                                                 lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "attention (Activation)          (None, 20, 20)       0           dot_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dot_2 (Dot)                     (None, 20, 512)      0           attention[0][0]                  \n",
            "                                                                 lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 20, 1024)     0           dot_2[0][0]                      \n",
            "                                                                 lstm_2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_1 (TimeDistrib (None, 20, 512)      524800      concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_2 (TimeDistrib (None, 20, 1941)     995733      time_distributed_1[0][0]         \n",
            "==================================================================================================\n",
            "Total params: 4,642,965\n",
            "Trainable params: 4,642,965\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hH4WN0-U4cvr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_enc_input = enc_train_input \n",
        "train_dec_input = np.zeros_like(enc_train_input)\n",
        "train_dec_input[:,1:] = enc_train_output[:,:-1]\n",
        "train_dec_input[:,0] = WORD_CODE_START\n",
        "\n",
        "train_dec_output=np.eye(dict_size)[enc_train_output.astype('int')]\n",
        "\n",
        "valid_enc_input = enc_valid_input\n",
        "valid_dec_input = np.zeros_like(enc_valid_output)\n",
        "valid_dec_input[:,1:] = enc_valid_output[:,:-1]\n",
        "valid_dec_input[:,0] = WORD_CODE_START\n",
        "valid_dec_output = np.eye(dict_size)[enc_valid_output.astype('int')]\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "U-Z6R0HtArSM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3451
        },
        "outputId": "c2c2fab7-bdd9-4430-f28a-24fd6d096afb"
      },
      "cell_type": "code",
      "source": [
        "model.fit(x=[train_enc_input, train_dec_input], y=[train_dec_output],\n",
        "         validation_data =([valid_enc_input, valid_dec_input], [valid_dec_output]),\n",
        "         batch_size=64, epochs=100)\n",
        "\n"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 24000 samples, validate on 6000 samples\n",
            "Epoch 1/100\n",
            "24000/24000 [==============================] - 64s 3ms/step - loss: 0.0031 - val_loss: 0.0028\n",
            "Epoch 2/100\n",
            "24000/24000 [==============================] - 54s 2ms/step - loss: 0.0027 - val_loss: 0.0026\n",
            "Epoch 3/100\n",
            "24000/24000 [==============================] - 54s 2ms/step - loss: 0.0025 - val_loss: 0.0025\n",
            "Epoch 4/100\n",
            "24000/24000 [==============================] - 54s 2ms/step - loss: 0.0024 - val_loss: 0.0024\n",
            "Epoch 5/100\n",
            "24000/24000 [==============================] - 54s 2ms/step - loss: 0.0024 - val_loss: 0.0024\n",
            "Epoch 6/100\n",
            "24000/24000 [==============================] - 54s 2ms/step - loss: 0.0023 - val_loss: 0.0024\n",
            "Epoch 7/100\n",
            "24000/24000 [==============================] - 54s 2ms/step - loss: 0.0023 - val_loss: 0.0023\n",
            "Epoch 8/100\n",
            "24000/24000 [==============================] - 54s 2ms/step - loss: 0.0023 - val_loss: 0.0023\n",
            "Epoch 9/100\n",
            "24000/24000 [==============================] - 54s 2ms/step - loss: 0.0023 - val_loss: 0.0023\n",
            "Epoch 10/100\n",
            "24000/24000 [==============================] - 54s 2ms/step - loss: 0.0022 - val_loss: 0.0023\n",
            "Epoch 11/100\n",
            "24000/24000 [==============================] - 54s 2ms/step - loss: 0.0022 - val_loss: 0.0023\n",
            "Epoch 12/100\n",
            "24000/24000 [==============================] - 54s 2ms/step - loss: 0.0022 - val_loss: 0.0023\n",
            "Epoch 13/100\n",
            "24000/24000 [==============================] - 54s 2ms/step - loss: 0.0021 - val_loss: 0.0023\n",
            "Epoch 14/100\n",
            "24000/24000 [==============================] - 54s 2ms/step - loss: 0.0021 - val_loss: 0.0023\n",
            "Epoch 15/100\n",
            "24000/24000 [==============================] - 54s 2ms/step - loss: 0.0021 - val_loss: 0.0023\n",
            "Epoch 16/100\n",
            "24000/24000 [==============================] - 54s 2ms/step - loss: 0.0020 - val_loss: 0.0023\n",
            "Epoch 17/100\n",
            "24000/24000 [==============================] - 54s 2ms/step - loss: 0.0020 - val_loss: 0.0023\n",
            "Epoch 18/100\n",
            "24000/24000 [==============================] - 54s 2ms/step - loss: 0.0020 - val_loss: 0.0024\n",
            "Epoch 19/100\n",
            "24000/24000 [==============================] - 54s 2ms/step - loss: 0.0019 - val_loss: 0.0024\n",
            "Epoch 20/100\n",
            "24000/24000 [==============================] - 54s 2ms/step - loss: 0.0018 - val_loss: 0.0024\n",
            "Epoch 21/100\n",
            "24000/24000 [==============================] - 56s 2ms/step - loss: 0.0018 - val_loss: 0.0025\n",
            "Epoch 22/100\n",
            "24000/24000 [==============================] - 55s 2ms/step - loss: 0.0017 - val_loss: 0.0025\n",
            "Epoch 23/100\n",
            "24000/24000 [==============================] - 55s 2ms/step - loss: 0.0017 - val_loss: 0.0025\n",
            "Epoch 24/100\n",
            "24000/24000 [==============================] - 54s 2ms/step - loss: 0.0016 - val_loss: 0.0026\n",
            "Epoch 25/100\n",
            "24000/24000 [==============================] - 54s 2ms/step - loss: 0.0015 - val_loss: 0.0026\n",
            "Epoch 26/100\n",
            "24000/24000 [==============================] - 54s 2ms/step - loss: 0.0014 - val_loss: 0.0027\n",
            "Epoch 27/100\n",
            "24000/24000 [==============================] - 55s 2ms/step - loss: 0.0014 - val_loss: 0.0028\n",
            "Epoch 28/100\n",
            "24000/24000 [==============================] - 54s 2ms/step - loss: 0.0013 - val_loss: 0.0028\n",
            "Epoch 29/100\n",
            "24000/24000 [==============================] - 54s 2ms/step - loss: 0.0012 - val_loss: 0.0029\n",
            "Epoch 30/100\n",
            "24000/24000 [==============================] - 54s 2ms/step - loss: 0.0012 - val_loss: 0.0030\n",
            "Epoch 31/100\n",
            "24000/24000 [==============================] - 55s 2ms/step - loss: 0.0011 - val_loss: 0.0030\n",
            "Epoch 32/100\n",
            "24000/24000 [==============================] - 54s 2ms/step - loss: 0.0010 - val_loss: 0.0031\n",
            "Epoch 33/100\n",
            "24000/24000 [==============================] - 54s 2ms/step - loss: 9.8076e-04 - val_loss: 0.0032\n",
            "Epoch 34/100\n",
            "24000/24000 [==============================] - 54s 2ms/step - loss: 9.2166e-04 - val_loss: 0.0032\n",
            "Epoch 35/100\n",
            "24000/24000 [==============================] - 54s 2ms/step - loss: 8.6460e-04 - val_loss: 0.0033\n",
            "Epoch 36/100\n",
            "24000/24000 [==============================] - 54s 2ms/step - loss: 8.1080e-04 - val_loss: 0.0034\n",
            "Epoch 37/100\n",
            "24000/24000 [==============================] - 55s 2ms/step - loss: 7.5577e-04 - val_loss: 0.0035\n",
            "Epoch 38/100\n",
            "24000/24000 [==============================] - 54s 2ms/step - loss: 7.0419e-04 - val_loss: 0.0035\n",
            "Epoch 39/100\n",
            "24000/24000 [==============================] - 54s 2ms/step - loss: 6.5474e-04 - val_loss: 0.0036\n",
            "Epoch 40/100\n",
            "24000/24000 [==============================] - 54s 2ms/step - loss: 6.0920e-04 - val_loss: 0.0037\n",
            "Epoch 41/100\n",
            "24000/24000 [==============================] - 54s 2ms/step - loss: 5.7088e-04 - val_loss: 0.0038\n",
            "Epoch 42/100\n",
            "24000/24000 [==============================] - 54s 2ms/step - loss: 5.2734e-04 - val_loss: 0.0038\n",
            "Epoch 43/100\n",
            "24000/24000 [==============================] - 54s 2ms/step - loss: 4.8385e-04 - val_loss: 0.0039\n",
            "Epoch 44/100\n",
            "24000/24000 [==============================] - 54s 2ms/step - loss: 4.4587e-04 - val_loss: 0.0040\n",
            "Epoch 45/100\n",
            "24000/24000 [==============================] - 54s 2ms/step - loss: 4.0806e-04 - val_loss: 0.0041\n",
            "Epoch 46/100\n",
            "24000/24000 [==============================] - 54s 2ms/step - loss: 3.7985e-04 - val_loss: 0.0041\n",
            "Epoch 47/100\n",
            "24000/24000 [==============================] - 54s 2ms/step - loss: 3.5285e-04 - val_loss: 0.0042\n",
            "Epoch 48/100\n",
            "24000/24000 [==============================] - 54s 2ms/step - loss: 3.1944e-04 - val_loss: 0.0043\n",
            "Epoch 49/100\n",
            "24000/24000 [==============================] - 54s 2ms/step - loss: 2.9263e-04 - val_loss: 0.0043\n",
            "Epoch 50/100\n",
            "24000/24000 [==============================] - 54s 2ms/step - loss: 2.7020e-04 - val_loss: 0.0044\n",
            "Epoch 51/100\n",
            "24000/24000 [==============================] - 54s 2ms/step - loss: 2.5791e-04 - val_loss: 0.0044\n",
            "Epoch 52/100\n",
            "24000/24000 [==============================] - 54s 2ms/step - loss: 2.4334e-04 - val_loss: 0.0045\n",
            "Epoch 53/100\n",
            "24000/24000 [==============================] - 54s 2ms/step - loss: 2.3238e-04 - val_loss: 0.0045\n",
            "Epoch 54/100\n",
            "24000/24000 [==============================] - 55s 2ms/step - loss: 2.1206e-04 - val_loss: 0.0046\n",
            "Epoch 55/100\n",
            "24000/24000 [==============================] - 55s 2ms/step - loss: 1.9657e-04 - val_loss: 0.0046\n",
            "Epoch 56/100\n",
            "24000/24000 [==============================] - 54s 2ms/step - loss: 1.8919e-04 - val_loss: 0.0047\n",
            "Epoch 57/100\n",
            "24000/24000 [==============================] - 54s 2ms/step - loss: 1.7634e-04 - val_loss: 0.0047\n",
            "Epoch 58/100\n",
            "24000/24000 [==============================] - 55s 2ms/step - loss: 1.6618e-04 - val_loss: 0.0047\n",
            "Epoch 59/100\n",
            "24000/24000 [==============================] - 55s 2ms/step - loss: 1.4878e-04 - val_loss: 0.0048\n",
            "Epoch 60/100\n",
            "24000/24000 [==============================] - 54s 2ms/step - loss: 1.4311e-04 - val_loss: 0.0048\n",
            "Epoch 61/100\n",
            "24000/24000 [==============================] - 54s 2ms/step - loss: 1.4977e-04 - val_loss: 0.0048\n",
            "Epoch 62/100\n",
            "24000/24000 [==============================] - 54s 2ms/step - loss: 1.5884e-04 - val_loss: 0.0049\n",
            "Epoch 63/100\n",
            "24000/24000 [==============================] - 54s 2ms/step - loss: 1.5906e-04 - val_loss: 0.0049\n",
            "Epoch 64/100\n",
            "24000/24000 [==============================] - 54s 2ms/step - loss: 1.5376e-04 - val_loss: 0.0049\n",
            "Epoch 65/100\n",
            "24000/24000 [==============================] - 54s 2ms/step - loss: 1.4276e-04 - val_loss: 0.0049\n",
            "Epoch 66/100\n",
            "24000/24000 [==============================] - 54s 2ms/step - loss: 1.2415e-04 - val_loss: 0.0050\n",
            "Epoch 67/100\n",
            "24000/24000 [==============================] - 54s 2ms/step - loss: 1.1190e-04 - val_loss: 0.0050\n",
            "Epoch 68/100\n",
            "24000/24000 [==============================] - 54s 2ms/step - loss: 1.0838e-04 - val_loss: 0.0050\n",
            "Epoch 69/100\n",
            "24000/24000 [==============================] - 54s 2ms/step - loss: 1.0858e-04 - val_loss: 0.0050\n",
            "Epoch 70/100\n",
            "24000/24000 [==============================] - 54s 2ms/step - loss: 1.1771e-04 - val_loss: 0.0051\n",
            "Epoch 71/100\n",
            "24000/24000 [==============================] - 54s 2ms/step - loss: 1.4007e-04 - val_loss: 0.0051\n",
            "Epoch 72/100\n",
            "24000/24000 [==============================] - 54s 2ms/step - loss: 1.5572e-04 - val_loss: 0.0051\n",
            "Epoch 73/100\n",
            "24000/24000 [==============================] - 55s 2ms/step - loss: 1.4316e-04 - val_loss: 0.0051\n",
            "Epoch 74/100\n",
            "24000/24000 [==============================] - 54s 2ms/step - loss: 1.1935e-04 - val_loss: 0.0051\n",
            "Epoch 75/100\n",
            "24000/24000 [==============================] - 55s 2ms/step - loss: 9.7072e-05 - val_loss: 0.0051\n",
            "Epoch 76/100\n",
            "24000/24000 [==============================] - 54s 2ms/step - loss: 8.3317e-05 - val_loss: 0.0051\n",
            "Epoch 77/100\n",
            "24000/24000 [==============================] - 54s 2ms/step - loss: 7.5524e-05 - val_loss: 0.0052\n",
            "Epoch 78/100\n",
            "24000/24000 [==============================] - 54s 2ms/step - loss: 8.5938e-05 - val_loss: 0.0052\n",
            "Epoch 79/100\n",
            "24000/24000 [==============================] - 55s 2ms/step - loss: 1.0940e-04 - val_loss: 0.0052\n",
            "Epoch 80/100\n",
            "24000/24000 [==============================] - 54s 2ms/step - loss: 1.4398e-04 - val_loss: 0.0052\n",
            "Epoch 81/100\n",
            "24000/24000 [==============================] - 54s 2ms/step - loss: 1.5699e-04 - val_loss: 0.0052\n",
            "Epoch 82/100\n",
            "24000/24000 [==============================] - 54s 2ms/step - loss: 1.3013e-04 - val_loss: 0.0052\n",
            "Epoch 83/100\n",
            "24000/24000 [==============================] - 54s 2ms/step - loss: 1.0173e-04 - val_loss: 0.0052\n",
            "Epoch 84/100\n",
            "24000/24000 [==============================] - 54s 2ms/step - loss: 7.7884e-05 - val_loss: 0.0052\n",
            "Epoch 85/100\n",
            "24000/24000 [==============================] - 54s 2ms/step - loss: 6.7365e-05 - val_loss: 0.0052\n",
            "Epoch 86/100\n",
            "24000/24000 [==============================] - 54s 2ms/step - loss: 6.1669e-05 - val_loss: 0.0053\n",
            "Epoch 87/100\n",
            "24000/24000 [==============================] - 54s 2ms/step - loss: 7.3578e-05 - val_loss: 0.0053\n",
            "Epoch 88/100\n",
            "24000/24000 [==============================] - 54s 2ms/step - loss: 1.1735e-04 - val_loss: 0.0053\n",
            "Epoch 89/100\n",
            "24000/24000 [==============================] - 54s 2ms/step - loss: 1.6638e-04 - val_loss: 0.0053\n",
            "Epoch 90/100\n",
            "24000/24000 [==============================] - 54s 2ms/step - loss: 1.5464e-04 - val_loss: 0.0053\n",
            "Epoch 91/100\n",
            "24000/24000 [==============================] - 54s 2ms/step - loss: 1.1426e-04 - val_loss: 0.0053\n",
            "Epoch 92/100\n",
            "24000/24000 [==============================] - 54s 2ms/step - loss: 8.2383e-05 - val_loss: 0.0053\n",
            "Epoch 93/100\n",
            "24000/24000 [==============================] - 54s 2ms/step - loss: 6.5132e-05 - val_loss: 0.0053\n",
            "Epoch 94/100\n",
            "24000/24000 [==============================] - 54s 2ms/step - loss: 5.9896e-05 - val_loss: 0.0053\n",
            "Epoch 95/100\n",
            "24000/24000 [==============================] - 54s 2ms/step - loss: 5.9866e-05 - val_loss: 0.0053\n",
            "Epoch 96/100\n",
            "24000/24000 [==============================] - 54s 2ms/step - loss: 7.3296e-05 - val_loss: 0.0054\n",
            "Epoch 97/100\n",
            "24000/24000 [==============================] - 54s 2ms/step - loss: 1.0852e-04 - val_loss: 0.0054\n",
            "Epoch 98/100\n",
            "24000/24000 [==============================] - 54s 2ms/step - loss: 1.5009e-04 - val_loss: 0.0053\n",
            "Epoch 99/100\n",
            "24000/24000 [==============================] - 54s 2ms/step - loss: 1.4749e-04 - val_loss: 0.0053\n",
            "Epoch 100/100\n",
            "24000/24000 [==============================] - 54s 2ms/step - loss: 1.1213e-04 - val_loss: 0.0053\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fa5abb56c18>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "metadata": {
        "id": "-m5CTooRB_EK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def prediction(raw_input):\n",
        "  clean_input = clean_text(raw_input)\n",
        "  input_tok = [nltk.word_tokenize(clean_input)]\n",
        "  input_tok = [input_tok[0][::-1]] #reversing input sequence\n",
        "  encoder_input = transform(encoding, input_tok,20)\n",
        "  decoder_input = np.zeros(shape=(len(encoder_input), OUTPUT_LENGTH))\n",
        "  decoder_input[:,0] = WORD_CODE_START\n",
        "  for i in range(1, OUTPUT_LENGTH):\n",
        "    output = model.predict([encoder_input,decoder_input]).argmax(axis=2)\n",
        "    decoder_input[:,i] = output[:,i]\n",
        "  return output\n",
        "\n",
        "def decode(decoding, vector):\n",
        "  text = ''\n",
        "  for i in vector:\n",
        "    if i==0:\n",
        "      break\n",
        "    text += ' '\n",
        "    text+= decoding[i]\n",
        "    \n",
        "  return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "J3yb8I9jfuCG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        },
        "outputId": "5f93d193-ba97-4b49-9b96-9957b97c5949"
      },
      "cell_type": "code",
      "source": [
        "for i in range(20):\n",
        "  seq_index = np.random.randint(1, len(short_questions))\n",
        "  output = prediction(short_questions[seq_index])\n",
        "  print('Q: ', short_questions[seq_index])\n",
        "  print('A: ', decode(decoding, output[0]))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Q:  no one to back me up. now i have a good idea. so unless you come up with something better\n",
            "A:   dignan . i can not do that . all right ? i just can not .\n",
            "Q:  how? what do they do for you, kaminski and his friends? they are all anarchists! traitors!\n",
            "A:   no , <UNK> . it is just <UNK> . some <UNK> <UNK> also . i am just\n",
            "Q:  well, do not do that again!\n",
            "A:   how are we going to live , <UNK> ? do you want me to go into me on ? ?\n",
            "Q:  you are happy for me and victor.\n",
            "A:   i can tell you guys are <UNK> what was the best part of our relationship\n",
            "Q:  no there had to be someone else. who knew?!\n",
            "A:   only me and dr. <UNK> .\n",
            "Q:  and why was the alarm on?\n",
            "A:   i could not just the fire <UNK> control at the other .\n",
            "Q:  barak is still strong in the north, and tendo holds the high passes. but the great desert forts have fallen.\n",
            "A:   oh . you seem about his <UNK> .\n",
            "Q:  if you say you are.\n",
            "A:   i most definitely say i am .\n",
            "Q:  he is out.\n",
            "A:   i know he is out , but how is he doing ?\n",
            "Q:  it is just not.\n",
            "A:   why not , you stupid bastard ?\n",
            "Q:  well you did it today.\n",
            "A:   yeah . i did it .\n",
            "Q:  you come! you speak first!\n",
            "A:   tell the chief we thank him .\n",
            "Q:  my birthday is april 10, 2015. how long do i live?\n",
            "A:   four years .\n",
            "Q:  lyssa? uwhyu?\n",
            "A:   yes , sir . he does not make <UNK> .\n",
            "Q:  it is a commercial area. no one lives there.\n",
            "A:   you will tell me .\n",
            "Q:  mom, tell him i am not going. you already promised me!\n",
            "A:   the guy 's <UNK> <UNK> than against the world .\n",
            "Q:  but it is a responsibility being a berserk.\n",
            "A:   i must only let the red <UNK> ...\n",
            "Q:  he fell. he falls.\n",
            "A:   he must have taken quite a fall .\n",
            "Q:  i cannot... i cannot move my arms. i hurt so much.\n",
            "A:   i know , but we can make it out of here . we can do it .\n",
            "Q:  what do you need?\n",
            "A:   i need ya help again .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "bybeJFsbgPdT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "1bdcd1ff-6923-4837-a5b6-fc42b1517721"
      },
      "cell_type": "code",
      "source": [
        "for i in range(5):\n",
        "  seq=input()\n",
        "  output = prediction(seq)\n",
        "  print('A: ', decode(decoding, output[0]))\n"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "my birthday is 28 jan 1997, how long do  I live?\n",
            "A:   four years .\n",
            "Inconsistent results yeah?\n",
            "A:   you are the <UNK> for east <UNK> , o'neil .\n",
            "my name is not o'neil\n",
            "A:   something new .\n",
            "something new\n",
            "A:   oh , thank you . have i <UNK> you ?\n",
            "fuck you\n",
            "A:   just a thought .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Hmx81sY2hED-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}